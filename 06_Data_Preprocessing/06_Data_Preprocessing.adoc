= Module 06 Data Preprocessing

* 데이터 분석의 정확도는 분석 데이터의 품질에 의해 좌우됨
* 누락 데이터, 중복 데이터 등 오류를 처리하고 분석 목적에 맞도록 변형해야 함
* 수집한 데이터를 분석에 적합하도록 하는 작업을 데이터 전처리(Data Pre-processing)이라고 함

== 누락 데이터 처리

* DataFrame에는 값이 누락되는 경우가 많음
* 파일 형식을 변경하는 과정에서 파일이 누락되는 경우가 많음
* 누락된 데이터는 NaN(Not a Number)로 표시함
* 누락 데이터가 많아지면 데이터의 품질이 떨어지고, 알고리즘을 왜곡하는 현상이 일어남

=== 누락 데이터 확인

[source, python]
----
import seaborn as sns

df_titanic = sns.load_dataset('titanic')
df_titanic.head()
----

[%header]
|===
|Survived|pclass|sex|age|sibsp|parch|fare|embarked|class|who|adult_male|deck|embark_town|alive|alone
|0	|3	|male	|22.0	|1	|0	|7.2500	|S	|Third	|man	|True	|NaN	|Southampton	|no	|False
|1	|1	|female	|38.0	|1	|0	|71.2833	|C	|First	|woman	|False	|C	|Cherbourg	|yes	|False
|1	|3	|female	|26.0	|0	|0	|7.9250	|S	|Third	|woman	|False	|NaN	|Southampton	|yes	|True
|1	|1	|female	|35.0	|1	|0	|53.1000	|S	|First	|woman	|False	|C	|Southampton	|yes	|False
|0	|3	|male	|35.0	|0	|0	|8.0500	|S	|Third	|man	|True	|NaN	|Southampton	|no	|True
|===

[source, python]
----
df_titanic.info()
----

* _age_, _embarked_, _deck_, _embark_town_ 에 누락 데이터가 있음

----
<class 'pandas.core.frame.DataFrame'>
RangeIndex: 891 entries, 0 to 890
Data columns (total 15 columns):
 #   Column       Non-Null Count  Dtype   
---  ------       --------------  -----   
 0   survived     891 non-null    int64   
 1   pclass       891 non-null    int64   
 2   sex          891 non-null    object  
 3   age          714 non-null    float64 
 4   sibsp        891 non-null    int64   
 5   parch        891 non-null    int64   
 6   fare         891 non-null    float64 
 7   embarked     889 non-null    object  
 8   class        891 non-null    category
 9   who          891 non-null    object  
 10  adult_male   891 non-null    bool    
 11  deck         203 non-null    category
 12  embark_town  889 non-null    object  
 13  alive        891 non-null    object  
 14  alone        891 non-null    bool    
dtypes: bool(2), category(2), float64(2), int64(4), object(5)
memory usage: 80.7+ KB
----

* _value_counts(dropna=False)_ 함수로 누락 데이터 확인

[source, python]
----
df_titanic.age.value_counts(dropna=False)
----
----
NaN      177
24.00     30
22.00     27
18.00     26
28.00     25
        ... 
36.50      1
55.50      1
0.92       1
23.50      1
74.00      1
Name: age, Length: 89, dtype: int64
----

* _isnull()_ 함수와 _notnull()_ 함수를 사용하여 누락 데이터 확인

[source, python]
----
df_titanic.embarked[df_titanic.embarked.isnull() == True]
df_titanic.embarked[df_titanic.embarked.notnull() == False]
----
----
61     NaN
829    NaN
Name: embarked, dtype: object
----
----
0      NaN
2      NaN
4      NaN
5      NaN
7      NaN
      ... 
884    NaN
885    NaN
886    NaN
888    NaN
890    NaN
Name: deck, Length: 688, dtype: category
Categories (7, object): ['A', 'B', 'C', 'D', 'E', 'F', 'G']
----

* isnull() 함수로 누락 데이터 개수 확인

[source, python]
----
df_titanic.deck.isnull().sum()
df_titanic.isnull().sum(axis=0)
----

=== 누락 데이터 삭제

* dropna() 함수에 thresh 값을 500으로 하여 누락 데이터가 500개 이상인 column을 삭제

[source, python]
----
df_titanic.dropna(axis=1, thresh=500, inplace=True)
----

* 분석에 영향을 크게 미치는 데이터라면, 삭제할 수 있음

[source, python]
----
df_age = df_titanic.dropna(subset=['age'], how='any', axis = 0)
len(df_age)
----
----
714
----

=== 누락 데이터 치환

* 데이터의 품질을 높일 목적으로 누락 데이터를 삭제하기 힘든 경우가 더 많음
* 데이터 분석의 정확도는 데이터의 품질외에도 제공되는 데이터의 양에 상당한 영향을 받음
* 누락 데이터를 대체할 값으로는 평균값, 최빈값 등을 사용
* _fillna()_ 함수 사용

_평균값을 사용하여 데이터 치환_

[source, python]
----
mean_age = df_titanic['age'].mean(axis=0)
df_titanic.fillna(mean_age, inplace=True)
----

_빈도수 높은 데이터를 사용하여 데이터 치환_
[source, python]
----
most_freq = df_titanic.embark_town.value_counts(dropna=True).idxmax()
df_titanic.embark_town.fillna(most_freq, inplace=True)
----

_이웃하고 있는 값으로 데이터 치환_
[source, python]
----
df_titanic.embark_town.fillna(method='ffill', inplace=True)
----

== 중복 데이터 처리

* DataFrame에서 각 행은 분석 대상이 가지고 있는 모든 속성에 대한 관측값을 뜻함
* 데이터셋에서 동일한 관측값이 2개 이상 중복되는 경우 중복 데이터를 찾아서 삭제해야 함
* 중복된 데이터는 분석을 왜곡함

=== 중복 데이터 확인

* duplicate() 메소드 사용
* 전에 나온 행들과 비교하여 중복되는 row면 True를 아니면 False를 반환함
* DataFrame에 duplicate() 메소드를 적용하면 각 row의 중복 여부를 나타내는 boolean Series를 반환

[source, python]
----
df = pd.DataFrame({
    'c1':['a','a','b','a','b'],
    'c2':[1,1,1,2,2],
    'c3':[1,1,1,2,2]
})

df_dup = df.duplicated()
col_dup = df.c2.duplicated()
col_dup = df[['c2','c3']].duplicated()
----

=== 중복 데이터 제거

* drop_duplicated() 메소드 사용
* 원본 객체에서 중복값을 제거하려면 inplace=True 옵션 사용
* subset 옵션으로 column 이름을 기준으로 삭제 가능

[source, python]
----
df.drop_duplicates(inplace=True)
df.drop_duplicates(subset=['c2','c3'], inplace=True)
----

== 데이터 표준화

* 실제 데이터는 단위 선택, 대소문자 구분, 약어 사용 등 여러 원인에 의해 다양한 형태로 표현됨
* 데이터의 형식이 다르면 분석의 정확도가 떨어짐
* 데이터 포맷을 일관성 있게 표준화하는 작업이 필요함

=== Sample Data

[%header, cols="1,1,1,1,1,1,1,2,1,3", width=100%]
|===
|	|mpg	|cylinders	|displayment	|horsepower	|weight	|acceleration	|model year	|origin	|name
|0	|18.0	|8	|307.0	|130.0	|3504.0	|12.0	|70	|1	|chevrolet chevelle malibu
|1	|15.0	|8	|350.0	|165.0	|3693.0	|11.5	|70	|1	|buick skylark 320
|2	|18.0	|8	|318.0	|150.0	|3436.0	|11.0	|70	|1	|plymouth satellite
|3	|16.0	|8	|304.0	|150.0	|3433.0	|12.0	|70	|1	|amc rebel sst
|...	|...	|...	|...	|...	|...	|...	|...	|...	|...
|395	|32.0	|4	|135.0	|84.00	|2295.0	|11.6	|82	|1	|dodge rampage
|396	|28.0	|4	|120.0	|79.00	|2625.0	|18.6	|82	|1	|ford ranger
|397	|31.0	|4	|119.0	|82.00	|2720.0	|19.4	|82	|1	|chevy s-10
|===

* 샘플 데이터에서, mpg는 갤런당 마일(mile per gallon)으로 표시됨

=== 단위 환산

* mpg를 kpl(kilo per liter)로 변환

[source, python]
----
mpg_to_kpl = 1.60934 / 3.78541
df_car['kpl'] = round(df_car['mpg'] * mpg_to_kpl, 2)
----

=== 자료형 변환

* horsepower 데이터는 누락 데이터를 '?' 로 표시함
* 결손치를 Numpy.non으로 변경후 누락 데이터를 삭제하고 타입을 변환

[source, python]
----
df_car['horsepower'].replace('?',np.nan, inplace=True)
df_car.dropna(subset=['horsepower'], axis=0, inplace=True)
df_car['horsepower'] = df_car['horsepower'].astype(float)
----

=== 카테고리형 데이터 처리

* 데이터 분석 알고리즘에 따라, 연속 데이터를 그대로 사용하는 것 보다는 일정한 구간으로 나누어 분석하는 것이 효율적인 경우가 많음
* 연속된 값을 일정한 구간으로 나누고 각 구간을 이산 변수로 변환하는 과정을 분할(binning)이라고 함
* cut() 함수를 사용하여 연속 데이터를 분할 데이터로 변환할 수 있음

[source, python]
----
count, bin_dividers = np.histogram(df_car['horsepower'], bins=3)
df_car['hp_bin'] = pd.cut(x=df_car['horsepower'],
                          bins=bin_dividers,
                          labels=['저출력','보통출력','고출력'],
                          include_lowest=True)
----

=== 정규화

* Dataframe의 column 데이터의 상대적 크기차이 때문에 분석 결과가 달라질 수 있음
* 숫자의 상대적인 크기 차이를 제거하면 더 정확한 분석 결과를 기대할 수 있음
* 각 column의 데이터를 동일한 크기 기준으로 나타내는 것을 정규화(normalization)이라고 함
** 데이터베이스의 정규화와는 다른 개념

=== 시계열 데이터

* Pandas는 주식, 환율 등 금융 데이터를 다루기 위한 기능을 제공함
* 시계열 데이터(time series)를 다룰 수 있는 유용한 기능들을 제공
* 시계열 데이터를 DataFrame의 row index로 사용하면 시간으로 기록된 데이터를 분석하기 용이함

==== Sample Data
[%header, cols=7, width=80%]
|===
|	|Date	|Close	|Start	|High	|Low	|Volume
|0	|2018-07-02	|10100	|10850	|10900	|10000	|137977
|1	|2018-06-29	|10700	|10550	|10900	|9990	|170253
|2	|2018-06-28	|10400	|10900	|10950	|10150	|155769
|3	|2018-06-27	|10900	|10800	|11050	|10500	|133548
|4	|2018-06-26	|10800	|10900	|11000	|10700	|63039
|===

==== Date를 시계열로 변환

* Date를 datetime으로 변환
* datetime 컬럼을 index로 지정하고 Date column 삭제

[source, python]
----
df_stock['new_date'] = pd.to_datetime(df_stock['Date'])
df_stock.set_index('new_date', inplace=True)
df_stock.drop('Date', axis=1, inplace=True)
----

=== 시계열 데이터 생성

* timestamp 배열 +
Pandas date_range() 함수를 사용하여 여러개의 날짜(timestamp)로 구성된 시계열 데이터 생성

[source, python]
----
ts_ms = pd.date_range(start='2019-01-01',   # 날짜 범위 시작
                      end=None,             # 날짜 범위 끝
                      periods=6,            # timestamp 개수
                      freq='MS',            # 시간 간격(MS: Month Start)
                      tz='Asia/Seoul'),     # 시간대

pr_h = pd.period_range(start='2019-01-01',
                       end=None,
                       periods=3,
                       freq='H')

pr_2h = pd.period_range(start='2019-01-01',
                       end=None,
                       periods=3,
                       freq='2H')
----

* freq: https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases 
* tz: pytz.all_timezones

=== 시계열 데이터 활용

* 연-월-일 날짜 데이터에서 일부를 분리하여 추출할 수 있음

[source, python]
----
df_stock['new_date'] = pd.to_datetime(df_stock['Date'])

df_stock['year'] = df_stock['new_date'].dt.year
df_stock['month'] = df_stock['new_date'].dt.month
df_stock['day'] = df_stock['new_date'].dt.day

df_stock['date_year'] = df_stock['new_date'].dt.to_period(freq='Y')
df_stock['date_month'] = df_stock['new_date'].dt.to_period(freq='M')
----

== 연습 문제

code 디렉토리의 WHO-COVID-19_global-data.csv 파일은 전 세계의 COVID-19 감염 통계 데이터를 가지고 있습니다.

[%header, cols="2,1,6", width=100%]
|===
|필드 이름|Type|설명
|Date_reported|Date|WHO에 보고돤 날짜
|Country_code|String|ISO Alpha-2 국가코드
|Country	   |String|국가, 지역, 영역
|WHO_region|String|WHO 지역 사무소: WHO 회원국은 아프리카 지역 사무소(AFRO), 미주 지역 사무소(AMRO), 동남아시아 지역 사무소(SEARO), 유럽 지역 사무소(EURO),동부 지중해 지역 사무소(EMRO) 및 서태평양 지역 사무소(WPRO) 등 6개의 WHO 지역으로 분류됩니다
|New_cases|Integer|새로 확인된 감염자 수. 현재 누적 사례에서 이전 누적 사례 수를 빼서 계산함
|Cumulative_cases|Integer|WHO에 보고돤 누적 확진자 수
|New_deaths|Integer|확인된 사망자 수
|Cumulative_deaths|Integer|WHO에 보고된 누적 사망자 수
|===

데이터를 이용하여 COVID-19 감염자/사망자 데이터를 분석하고 보고서를 작성하세요.